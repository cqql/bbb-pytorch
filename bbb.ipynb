{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    torchvision.transforms.Lambda(lambda im: im.reshape(-1))\n",
    "])\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, transform=mnist_transforms, download=True)\n",
    "mnist_test = mnist_train = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=False, transform=mnist_transforms, download=True)\n",
    "\n",
    "mnist_loader = torch.utils.data.DataLoader(mnist_train, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_train, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the classification model\n",
    "\n",
    "This is the same model used by the author's in section 5.1 of the paper. It has two hidden layers of 1200 units each, RELU activations and softmax outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weights(nn.Module):\n",
    "    MU = 0.0\n",
    "    SIGMA = np.log(1 + np.exp(-5.0))\n",
    "    \n",
    "    def __init__(self, n, m):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mu = nn.Parameter(torch.zeros(n, m))\n",
    "        rho_init = torch.zeros(n, m)\n",
    "        rho_init.fill_(-5.0)\n",
    "        self.rho = nn.Parameter(rho_init)\n",
    "        \n",
    "    def sample(self):\n",
    "        epsilon = torch.randn_like(self.mu)\n",
    "        \n",
    "        return self.mu + epsilon * F.softplus(self.rho)\n",
    "    \n",
    "    def kl_divergence(self, w):\n",
    "        # Compute only the parts that influence the gradient\n",
    "        sigma = F.softplus(self.rho)\n",
    "        return 0.5 * ((w - self.MU)**2 / self.SIGMA - (w - self.mu)**2 / sigma - sigma.log()).sum()\n",
    "        \n",
    "\n",
    "class BBB(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.w1 = Weights(1200, 784)\n",
    "        self.w2 = Weights(1200, 1200)\n",
    "        self.wo = Weights(10, 1200)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(F.linear(X, self.w1.sample()))\n",
    "        X = F.relu(F.linear(X, self.w2.sample()))\n",
    "        X = F.linear(X, self.wo.sample())\n",
    "        \n",
    "        return F.softmax(X, dim=-1)\n",
    "    \n",
    "    def elbo(self, X, y, dataset_size):\n",
    "        w1 = self.w1.sample()\n",
    "        w2 = self.w2.sample()\n",
    "        wo = self.wo.sample()\n",
    "        kld = self.w1.kl_divergence(w1) + self.w2.kl_divergence(w2) + self.wo.kl_divergence(wo)\n",
    "        \n",
    "        X = F.relu(F.linear(X, self.w1.sample()))\n",
    "        X = F.relu(F.linear(X, self.w2.sample()))\n",
    "        logits = F.linear(X, self.wo.sample())\n",
    "        \n",
    "        max, _ = logits.max(dim=-1, keepdim=True)\n",
    "        o = (logits - max).exp()\n",
    "        log_likelihood = (torch.gather(logits, -1, y.view(-1, 1)) - max - torch.log(o.sum(dim=-1))).sum()\n",
    "        \n",
    "        return (len(logits) / dataset_size) * kld - log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: accuracy 0.09 loss 114309.40625\n",
      "Batch 10: accuracy 0.61 loss 89058.25\n",
      "Batch 20: accuracy 0.84 loss 87201.390625\n",
      "Batch 30: accuracy 0.87 loss 90258.3671875\n",
      "Batch 40: accuracy 0.90 loss 82404.0625\n",
      "Batch 50: accuracy 0.91 loss 78465.9921875\n",
      "Batch 60: accuracy 0.93 loss 82300.828125\n",
      "Batch 70: accuracy 0.93 loss 80534.3671875\n",
      "Batch 0: accuracy 0.93 loss 82542.3359375\n",
      "Batch 10: accuracy 0.94 loss 76018.8125\n",
      "Batch 20: accuracy 0.92 loss 77017.34375\n",
      "Batch 30: accuracy 0.94 loss 77417.390625\n",
      "Batch 40: accuracy 0.94 loss 74072.390625\n",
      "Batch 50: accuracy 0.95 loss 73377.65625\n",
      "Batch 60: accuracy 0.95 loss 76385.9453125\n",
      "Batch 70: accuracy 0.93 loss 73083.828125\n"
     ]
    }
   ],
   "source": [
    "bbb = BBB()\n",
    "optimizer = torch.optim.Adam(bbb.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for batch, data in enumerate(mnist_loader):\n",
    "        imgs, labels = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = bbb.elbo(imgs, labels, len(mnist_train))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            imgs, labels = next(iter(test_loader))\n",
    "            classifications = bbb.forward(imgs).argmax(dim=-1)\n",
    "            accuracy = float((classifications == labels).sum()) / len(labels)\n",
    "            print(f\"Batch {batch}: accuracy {accuracy:.2f} loss {float(loss)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
